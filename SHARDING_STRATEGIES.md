# GT Sharding Strategies

This document explains how distributed matrix operations work in GT.

## Strategy 1: Row-Sharded Matmul (Current Implementation)

**Use case**: `A @ B` where A is row-sharded

### Sharding Pattern
```
A (128, 64) sharded along axis 0 (rows)
‚Üí Worker 0: A‚ÇÄ (32, 64)  [rows 0-31]
‚Üí Worker 1: A‚ÇÅ (32, 64)  [rows 32-63]
‚Üí Worker 2: A‚ÇÇ (32, 64)  [rows 64-95]
‚Üí Worker 3: A‚ÇÉ (32, 64)  [rows 96-127]

B (64, 32) broadcast to all workers
‚Üí All workers: B (64, 32)
```

### Computation
Each worker computes independently:
```
Worker 0: C‚ÇÄ = A‚ÇÄ @ B = (32, 64) @ (64, 32) = (32, 32)
Worker 1: C‚ÇÅ = A‚ÇÅ @ B = (32, 64) @ (64, 32) = (32, 32)
Worker 2: C‚ÇÇ = A‚ÇÇ @ B = (32, 64) @ (64, 32) = (32, 32)
Worker 3: C‚ÇÉ = A‚ÇÉ @ B = (32, 64) @ (64, 32) = (32, 32)
```

### Communication
**Gather** (concatenate):
```
C = [C‚ÇÄ; C‚ÇÅ; C‚ÇÇ; C‚ÇÉ] = (128, 32)
```

**No all-reduce needed!** Each worker computed a different part of the output.

### Implementation
- Broadcast B to all workers (one-time cost)
- Each worker computes local matmul
- Dispatcher gathers results by concatenating along axis 0
- See: `_handle_distributed_matmul()` in `dispatcher.py`

---

## Strategy 2: Reduction-Dimension Sharding (Requires All-Reduce)

**Use case**: `sum(A)` or `mean(A)` where A is sharded

### Example: sum() on Sharded Tensor

```
A (128, 64) sharded along axis 0
‚Üí Worker 0: A‚ÇÄ (32, 64)
‚Üí Worker 1: A‚ÇÅ (32, 64)
‚Üí Worker 2: A‚ÇÇ (32, 64)
‚Üí Worker 3: A‚ÇÉ (32, 64)
```

### Computation
Each worker computes partial sum:
```
Worker 0: s‚ÇÄ = sum(A‚ÇÄ) = scalar
Worker 1: s‚ÇÅ = sum(A‚ÇÅ) = scalar
Worker 2: s‚ÇÇ = sum(A‚ÇÇ) = scalar
Worker 3: s‚ÇÉ = sum(A‚ÇÉ) = scalar
```

### Communication
**All-reduce (sum)**:
```
result = s‚ÇÄ + s‚ÇÅ + s‚ÇÇ + s‚ÇÉ
```

All workers receive the final result.

### Why All-Reduce?
- Each worker has only **part** of the data
- The final result depends on **combining** all partial results
- Unlike matmul, we can't just concatenate - we need to SUM

---

## Strategy 3: Column-Sharded Matmul (Future - Also Needs All-Reduce)

**Use case**: `A @ B` where B is column-sharded along inner dimension

### Sharding Pattern
```
A (128, 64) sharded along axis 1 (columns)
‚Üí Worker 0: A‚ÇÄ (128, 16)  [cols 0-15]
‚Üí Worker 1: A‚ÇÅ (128, 16)  [cols 16-31]
‚Üí Worker 2: A‚ÇÇ (128, 16)  [cols 32-47]
‚Üí Worker 3: A‚ÇÉ (128, 16)  [cols 48-63]

B (64, 32) sharded along axis 0 (rows - same as A's columns!)
‚Üí Worker 0: B‚ÇÄ (16, 32)  [rows 0-15]
‚Üí Worker 1: B‚ÇÅ (16, 32)  [rows 16-31]
‚Üí Worker 2: B‚ÇÇ (16, 32)  [rows 32-47]
‚Üí Worker 3: B‚ÇÉ (16, 32)  [rows 48-63]
```

### Computation
Each worker computes partial matmul:
```
Worker 0: C‚ÇÄ = A‚ÇÄ @ B‚ÇÄ = (128, 16) @ (16, 32) = (128, 32)
Worker 1: C‚ÇÅ = A‚ÇÅ @ B‚ÇÅ = (128, 16) @ (16, 32) = (128, 32)
Worker 2: C‚ÇÇ = A‚ÇÇ @ B‚ÇÇ = (128, 16) @ (16, 32) = (128, 32)
Worker 3: C‚ÇÉ = A‚ÇÉ @ B‚ÇÉ = (128, 16) @ (16, 32) = (128, 32)
```

### Communication
**All-reduce (sum)**:
```
C = C‚ÇÄ + C‚ÇÅ + C‚ÇÇ + C‚ÇÉ = (128, 32)
```

This is the **reduction over the inner dimension (k)**!

---

## Implementation Status

‚úÖ **Implemented**:
- Row-sharded matmul (Strategy 1)
- Auto-sharding for tensor creation (randn, zeros)
- Gather-based result collection

üöß **In Progress**:
- Engine abstraction (numpy vs pytorch)
- Distributed sum/mean with all-reduce

‚è≥ **TODO**:
- Column-sharded matmul (Strategy 3)
- PyTorch distributed backend integration (NCCL)
- Automatic sharding strategy selection

---

## When to Use Each Strategy

| Operation | Input Sharding | Strategy | Communication |
|-----------|---------------|----------|---------------|
| `A @ B` | A row-sharded | 1 (current) | Broadcast B, Gather result |
| `sum(A)`, `mean(A)` | A sharded any axis | 2 (in progress) | All-reduce sum |
| `A @ B` | Both col-sharded on inner dim | 3 (future) | All-reduce sum |

---

## PyTorch Distributed Primitives

The `PyTorchEngine` provides distributed operations:

```python
# All-reduce sum (Strategy 2 & 3)
tensor = engine.all_reduce_sum(tensor, group=None)

# Future: other collectives
tensor = engine.all_gather(tensor, group=None)  # Gather to all workers
tensor = engine.reduce_scatter(tensor, group=None)  # Reduce and scatter
```

Only PyTorch engine supports distributed ops (`engine.supports_distributed() == True`).

NumPy engine will raise `NotImplementedError` for distributed operations.
