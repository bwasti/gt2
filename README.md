# GT2 - Graph Tensor System

**A distributed tensor framework with PyTorch-like API that scales from 1 to 72+ GPUs.**

Key features:
- ğŸš€ **5000x speedup** with instruction batching + torch.compile()
- ğŸ¯ **Signal-based sharding** - Configure data/model/pipeline parallelism via YAML
- ğŸ”¥ **Full autograd** - Tape-based automatic differentiation
- âš¡ **Zero code changes** to scale across GPUs
- ğŸ“ **Simple & readable** - Designed for ML researchers

## ğŸ¯ Quick Start

```python
import gt

# Auto-starts local server - no setup needed!
a = gt.randn(1000, 1000)
b = gt.randn(1000, 1000)
c = a @ b
result = c.data.numpy()
```

That's it! For distributed training, see [Distributed Setup](#distributed-setup).

## âœ¨ Key Features

### ğŸš€ Instruction Batching & torch.compile (5000x Speedup!)

Workers batch operations and compile them with `torch.compile()` for dramatic speedups:

```bash
# Enable batching (default: eager mode)
GT_WORKER_BATCH_SIZE=10 python your_script.py
```

**Performance:**
- First compilation: ~10s (one-time cost)
- Cached execution: **~1.5ms** (5000x faster!)
- Identical computation patterns reuse cached compiled functions

See [docs/BATCHING_AND_COMPILATION.md](docs/BATCHING_AND_COMPILATION.md) for details.

### ğŸ¯ Signal-Based Sharding Configuration

Control tensor placement across workers using named signals and YAML configs:

```yaml
# config.yaml
forward_layer1:
  shard:
    axis: 0                  # Shard batch dimension
    workers: [0, 1, 2, 3]    # Use all 4 GPUs
  backward: backward_layer1  # Different config for gradients

pipeline_stage_1:
  shard:
    axis: 0
    workers: [0, 1]          # Stage 1 on first 2 GPUs
  backward: pipeline_stage_1_bwd

pipeline_stage_1_bwd:
  shard:
    axis: 0
    workers: [2, 3]          # Backward on last 2 GPUs (pipeline parallelism!)
```

```python
import os
os.environ['GT_CONFIG'] = 'config.yaml'
import gt

# Context manager - shards tensors AND compute
with gt.signal.context('forward_layer1'):
    x = gt.randn(100, 64)  # Sharded across workers [0,1,2,3]
    y = x @ w              # Compute happens in sharded mode

# Function call - copy-shard tensor only
x_sharded = gt.signal.tensor(x, name='forward_layer1')

# Pipeline parallelism - forward/backward on different workers
with gt.signal.context('pipeline_stage_1', backward='pipeline_stage_1_bwd'):
    loss = model(input)
    loss.backward()  # Gradients computed on workers [2,3]
```

**Supports:**
- Data parallelism (batch sharding)
- Model parallelism (feature sharding)
- Pipeline parallelism (stage-wise worker assignment)
- Replicated parameters
- Per-layer sharding strategies
- Compilation directives (`compile: true` for torch.compile boundaries)

See [examples/README_SIGNALS.md](examples/README_SIGNALS.md) for comprehensive guide.

### ğŸ”¥ Full Autograd Support

Train neural networks with automatic differentiation:

```python
from gt.nn import Module, Linear, SGD

class MLP(Module):
    def __init__(self):
        super().__init__()
        self.fc1 = Linear(784, 256)
        self.fc2 = Linear(256, 10)

    def forward(self, x):
        x = self.fc1(x).relu()
        return self.fc2(x)

# Training loop
model = MLP()
optimizer = SGD(model.parameters(), lr=0.01)

for epoch in range(100):
    # Forward pass
    pred = model(X_train)
    loss = ((pred - y_train) ** 2).mean()

    # Backward pass
    loss.backward()

    # Update parameters
    optimizer.step()
    optimizer.zero_grad()

    print(f"Epoch {epoch}: loss = {loss.item():.4f}")
# Output: Epoch 99: loss = 0.0274 âœ…
```

**Features:**
- Tape-based autograd (like PyTorch)
- Full gradient computation with broadcasting
- In-place parameter updates
- Optimizers: SGD (more coming soon)

### ğŸ“Š Instruction Stream Logging

Debug and profile with comprehensive operation logging:

```bash
GT_INSTRUCTION_LOG=ops.log python your_script.py
```

Output shows every operation with timestamps:
```
[TAPE 0.003s] RECV         | 127.0.0.1:59712 | BinaryOp  | result=2 op=matmul left=0 right=1
[TAPE 0.003s] WORKER_SEND  | auto_worker     | WorkerOp  | op=matmul result=127.0.0.1:59712_2
[TAPE 0.004s] WORKER_RECV  | auto_worker     | WorkerOp  | success=True
[TAPE 0.004s] SEND         | 127.0.0.1:59712 | BinaryOp  | success=True
```

Perfect for:
- Debugging hangs (see the last operation before timeout)
- Identifying slow operations (large timestamp gaps)
- Understanding distributed execution flow

### âš¡ Multiple Backends

- **PyTorch** (default): Fast, supports GPU, compilation, distributed primitives
- **NumPy**: CPU-only reference implementation for debugging

Workers automatically select PyTorch when batching is enabled.

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/bwasti/gt2.git
cd gt2

# Install dependencies
pip install -r requirements.txt

# Optional: Install PyTorch for GPU support
pip install torch
```

## ğŸš€ Usage

### Simple (Auto-Server)

Perfect for local development and single-GPU workloads:

```python
import gt

# Auto-starts local server with 1 worker
a = gt.tensor([1, 2, 3, 4])
b = gt.tensor([5, 6, 7, 8])
c = a + b
data = c.data.numpy()  # [6, 8, 10, 12]
```

Configure number of workers:
```python
import gt
gt.gpu_workers(4)  # Use 4 workers (must call before any tensor operations)

# Now tensors can be sharded across 4 workers
a = gt.randn(1000, 1000)
```

### Distributed Setup

For production or multi-GPU/multi-node clusters:

**Terminal 1 - Start dispatcher:**
```bash
python -m gt.server -p 12345
```

**Terminal 2-N - Start workers (1 per GPU):**
```bash
# Worker 0 (GPU 0)
CUDA_VISIBLE_DEVICES=0 python -m gt.worker --host localhost -p 12345

# Worker 1 (GPU 1)
CUDA_VISIBLE_DEVICES=1 python -m gt.worker --host localhost -p 12345

# Worker 2 (GPU 2) on another machine
CUDA_VISIBLE_DEVICES=0 python -m gt.worker --host <dispatcher_host> -p 12345
```

**Terminal N - Run your code:**
```python
import gt

gt.connect('localhost:12345')
a = gt.randn(10000, 10000)
b = gt.randn(10000, 10000)
c = a @ b  # Computed across all connected workers
```

Scale to **72+ GPUs across 36 nodes** by starting more workers!

## ğŸ§ª Environment Variables

Configure GT behavior via environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `GT_WORKER_BATCH_SIZE` | Batch size for instruction batching | `1` (eager) |
| `GT_CONFIG` | Path to sharding config YAML | None |
| `GT_INSTRUCTION_LOG` | Path to instruction stream log file | None |

**Example:**
```bash
GT_WORKER_BATCH_SIZE=10 \
GT_CONFIG=sharding.yaml \
GT_INSTRUCTION_LOG=debug.log \
python train.py
```

## ğŸ“š API Reference

### Tensor Operations

All standard operations supported with autograd:

**Arithmetic:** `+`, `-`, `*`, `/`, `@` (matmul)
**Activations:** `relu()`, `sigmoid()`, `tanh()`
**Reductions:** `sum()`, `mean()`
**Math:** `exp()`, `log()`
**Shape:** `transpose()`, `.T`
**In-place:** `-=`, `zero_()`

### Neural Network Modules

```python
from gt.nn import Module, Linear, SGD

# Define models
class MyModel(Module):
    def __init__(self):
        super().__init__()
        self.layer1 = Linear(input_size, hidden_size)
        self.layer2 = Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.layer1(x).relu()
        return self.layer2(x)

# Optimizers
optimizer = SGD(model.parameters(), lr=0.01)
```

### Autograd

```python
# Enable gradients
a = gt.tensor([1.0, 2.0, 3.0], requires_grad=True)
b = gt.from_numpy(np.array(...), requires_grad=True)

# Forward pass
loss = (a * 2).sum()

# Backward pass
loss.backward()

# Access gradients
print(a.grad.data.numpy())  # [2, 2, 2]
```

### Signal-Based Sharding

```python
# Load config
gt.load_config('sharding.yaml')
# Or use environment variable:
# os.environ['GT_CONFIG'] = 'sharding.yaml'

# Context manager
with gt.signal.context('layer1'):
    x = gt.randn(100, 64)

# Function call
x_sharded = gt.signal.tensor(x, name='layer1')

# Enter/exit API
gt.signal.enter('layer1')
x = gt.randn(100, 64)
gt.signal.exit('layer1')
```

## ğŸ§ª Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test suite
pytest tests/test_basic_ops.py -v
pytest tests/test_autograd.py -v
pytest tests/test_mlp.py -v

# Test with batching enabled
GT_WORKER_BATCH_SIZE=10 pytest tests/ -v
```

All tests auto-start a local GT system and verify numeric correctness.

## ğŸ“Š Benchmarks

```bash
cd benchmarks
python compare.py
```

**Note:** GT adds communication/serialization overhead. For small operations this can be significant. For large operations (training, big matmuls), overhead becomes negligible and batching/compilation provide massive speedups.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          User Code                               â”‚
â”‚  import gt                                                       â”‚
â”‚  with gt.signal.context('layer1'):                              â”‚
â”‚      x = gt.randn(100, 64)                                      â”‚
â”‚      loss = model(x)                                            â”‚
â”‚      loss.backward()                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ PyTorch-like API + Signal Metadata
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      gt/client/                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Tensor     â”‚  â”‚  Autograd   â”‚  â”‚  nn.Module   â”‚          â”‚
â”‚  â”‚ (Remote Data)â”‚  â”‚   (Tape)    â”‚  â”‚  (Layers)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Signal API: Tracks current signal scope         â”‚           â”‚
â”‚  â”‚ Config: Loads YAML sharding strategies          â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ TCP (Protocol with signal metadata)
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    gt/dispatcher/                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  â€¢ Reads signal configs from YAML                  â”‚        â”‚
â”‚  â”‚  â€¢ Routes operations based on sharding strategy    â”‚        â”‚
â”‚  â”‚  â€¢ Logs instruction stream to file                 â”‚        â”‚
â”‚  â”‚  â€¢ Handles multiple clients concurrently           â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚ TCP (WorkerProtocol)
        â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚Worker 0â”‚    â”‚Worker 1â”‚    â”‚Worker Nâ”‚ (1 per GPU)
    â”‚        â”‚    â”‚        â”‚    â”‚        â”‚
    â”‚Batchingâ”‚    â”‚Batchingâ”‚    â”‚Batchingâ”‚ Instruction batching
    â”‚Compile â”‚    â”‚Compile â”‚    â”‚Compile â”‚ torch.compile caching
    â”‚        â”‚    â”‚        â”‚    â”‚        â”‚
    â”‚PyTorch â”‚    â”‚PyTorch â”‚    â”‚PyTorch â”‚ Backend
    â”‚  GPU   â”‚    â”‚  GPU   â”‚    â”‚  GPU   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Host 0        Host 0        Host N
```

**Key Components:**

- **`gt/client/`** - User-facing API
  - Location-transparent tensors
  - Tape-based autograd
  - Signal tracking and scope management
  - Neural network modules

- **`gt/dispatcher/`** - Coordinates clients and schedules operations
  - Maps `client:tensor` to physical locations
  - Reads signal configs for sharding decisions
  - Instruction stream logging
  - Handles multiple concurrent clients

- **`gt/worker/`** - Executes operations with backends
  - **Instruction batching**: Accumulates ops before execution
  - **torch.compile()**: Compiles and caches computation graphs
  - **Multiple backends**: PyTorch (with GPU) or NumPy (reference)
  - One worker per GPU

- **`gt/signal.py`** - Signal-based sharding API
  - Context managers for scope tracking
  - Thread-local signal stack
  - Backward signal support for pipeline parallelism

- **`gt/config.py`** - YAML config loading
  - Parses sharding strategies
  - Maps signal names to worker assignments

## ğŸ“– Documentation

- [Signal-Based Sharding Guide](examples/README_SIGNALS.md) - Complete guide to sharding API
- [Batching & Compilation](docs/BATCHING_AND_COMPILATION.md) - How instruction batching works
- [CLAUDE.md](CLAUDE.md) - Detailed architecture documentation

## ğŸ¯ Design Philosophy

This code is designed to be **SIMPLE and READABLE** for ML researchers. We prioritize:

1. **Clarity over performance** in the initial implementation
2. **PyTorch-compatible API** for easy adoption
3. **Declarative configuration** via YAML instead of complex APIs
4. **Automatic optimization** via torch.compile (no manual graph building)

## ğŸš€ Examples

See [examples/](examples/) directory:

- `signal_demo.py` - Signal-based sharding demonstration
- `compile_demo.py` - Compilation directives demonstration
- `config_sharding.yaml` - Example sharding configuration
- `config_compile.yaml` - Example compilation configuration
- `demo.py` - Basic tensor operations
- `simple_launch.py` - Manual server/worker launch

## ğŸ“Š Performance Tips

1. **Enable batching** for training workloads:
   ```bash
   GT_WORKER_BATCH_SIZE=10 python train.py
   ```

2. **Use signals** to control sharding strategies:
   ```python
   with gt.signal.context('data_parallel'):
       # Batch-sharded across all workers
       x = gt.randn(1000, 1000)
   ```

3. **Warmup compilation** before timing:
   ```python
   # Run once to compile
   for i in range(3):
       loss = model(X)
       loss.backward()

   # Now time actual training
   start = time.time()
   for epoch in range(100):
       loss = model(X)
       loss.backward()
   print(f"Time: {time.time() - start:.2f}s")
   ```

4. **Use instruction logging** to find bottlenecks:
   ```bash
   GT_INSTRUCTION_LOG=profile.log python train.py
   # Check profile.log for large timestamp gaps
   ```

## ğŸ¤ Contributing

Contributions welcome! This is a research prototype focused on simplicity and readability.

## ğŸ“ License

MIT

---

**Built with â¤ï¸ for ML researchers who want distributed training without the complexity.**
